{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "structured-tuning"
      },
      "source": [
        "# PyTorch 2D convolutions\n",
        "#### Advanced Deep Learning, 2024"
      ],
      "id": "structured-tuning"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooBCszdvpKZs"
      },
      "source": [
        "## mount driver and establish workspace"
      ],
      "id": "ooBCszdvpKZs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6YBvOQDpVg3"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "id": "Q6YBvOQDpVg3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3LgNo85paJH"
      },
      "source": [
        "## go to your folder\n",
        "%cd /content/drive/My Drive/Colab Notebooks"
      ],
      "id": "F3LgNo85paJH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "realistic-banana"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "id": "realistic-banana",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sized-craft"
      },
      "source": [
        "## One input channel, one output, no padding\n",
        "Let's define a `W`$\\times$`W` filter. For the following examples, we do not need a bias parameter."
      ],
      "id": "sized-craft"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "capital-granny"
      },
      "source": [
        "# Convolution filter is of size W\n",
        "W = 3\n",
        "# 1 input (image) channel, 1 output channel, WxW convolution kernel\n",
        "conv = nn.Conv2d(1, 1, W, bias=False)\n",
        "print(\"We just defined:\", conv)"
      ],
      "id": "capital-granny",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "virgin-throat"
      },
      "source": [
        "Let's look at the kernel dimensions:"
      ],
      "id": "virgin-throat"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "homeless-discovery"
      },
      "source": [
        "# 1 output channel, 1 input channel, 1st dimension = W, 2nd dimension = W\n",
        "print(conv.weight.shape)"
      ],
      "id": "homeless-discovery",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ordered-working"
      },
      "source": [
        "The filter parameters are initialized randomly:"
      ],
      "id": "ordered-working"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "excess-pennsylvania"
      },
      "source": [
        "print(conv.weight)"
      ],
      "id": "excess-pennsylvania",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lonely-monaco"
      },
      "source": [
        "We can set the parameters as follows:"
      ],
      "id": "lonely-monaco"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ambient-sender"
      },
      "source": [
        "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
        "print(conv.weight)"
      ],
      "id": "ambient-sender",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "serial-evolution"
      },
      "source": [
        "Let's define an input (image) `x`. The input is of the same shape as the filter:"
      ],
      "id": "serial-evolution"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "preceding-spank"
      },
      "source": [
        "x = torch.arange(float(W*W))\n",
        "x = torch.reshape(x, (1, 1, W, W))\n",
        "print('Input:\\n', x)\n",
        "print('Sum of all input elements:', torch.sum(x).item())"
      ],
      "id": "preceding-spank",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "streaming-benjamin"
      },
      "source": [
        "Because there is no padding and input and filter have the same size, there is only one valid position for the filter. Accordingly, the result is a tensor with a single value:"
      ],
      "id": "streaming-benjamin"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "million-bunch"
      },
      "source": [
        "c = conv(x)\n",
        "print('Tensor:', c, 'scalar:', c.item())"
      ],
      "id": "million-bunch",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final-positive"
      },
      "source": [
        "The scalar should be equal to the sum of all input elements (ensure that you understand why)."
      ],
      "id": "final-positive"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "limiting-stomach"
      },
      "source": [
        "## One input channel, one output,  padding\n",
        "Now we add zero-padding such that the input dimensionality is preseved:\n"
      ],
      "id": "limiting-stomach"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naughty-alfred"
      },
      "source": [
        "conv = nn.Conv2d(1, 1, W, padding=W//2, bias=False)\n",
        "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
        "c = conv(x)\n",
        "print(c)"
      ],
      "id": "naughty-alfred",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "invalid-communication"
      },
      "source": [
        "## Several input channels, one output, no padding\n",
        "Typically, the input to a convolutional layer consists of several feature maps or channels. For example, consider a 2D input with three channels (e.g., an RGB colour image):"
      ],
      "id": "invalid-communication"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amber-somerset"
      },
      "source": [
        "x = torch.arange(float(3*W*W))\n",
        "x = torch.reshape(x, (1, 3, W, W))\n",
        "print('Input:', x)\n",
        "print('Sum of all inputs:', torch.sum(x).item())"
      ],
      "id": "amber-somerset",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voluntary-porter"
      },
      "source": [
        "Let's define a convolutional layer that takes three channels as input and produces a single output feature map:"
      ],
      "id": "voluntary-porter"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spare-ranch"
      },
      "source": [
        "# 3 input (image) channels, 1 output channel, WxW convolution kernel\n",
        "conv = nn.Conv2d(3, 1, W, bias=False)\n",
        "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
        "print('Weight parameters of convolutional layer:', conv.weight)"
      ],
      "id": "spare-ranch",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prime-wrapping"
      },
      "source": [
        "Note that there is one filter for each input channel.\n",
        "The convolutional layer first convolves each input channel with the corresponding filter.\n",
        "This results in three feature maps, whih are added to give the final result:"
      ],
      "id": "prime-wrapping"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "remarkable-prayer"
      },
      "source": [
        "c = conv(x)\n",
        "print('number of filter parameters:', conv.weight.numel(), '\\nresult of filtering the input:', c)"
      ],
      "id": "remarkable-prayer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accessible-calcium"
      },
      "source": [
        "It is important that the number of parameters and the dimesionality of the result is clear to you."
      ],
      "id": "accessible-calcium"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expected-antarctica"
      },
      "source": [
        "Now let's apply 1$\\times$1 convolutions to our three input channels. Again, we set all filter weights to 1."
      ],
      "id": "expected-antarctica"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "likely-front"
      },
      "source": [
        "# 3 input (image) channels, 1 output channel, 1x1 convolution kernel\n",
        "conv = nn.Conv2d(3, 1, 1, bias=False)\n",
        "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
        "print(conv.weight)"
      ],
      "id": "likely-front",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advisory-resort"
      },
      "source": [
        "This convolutional layer adds the three input feature maps/channels:"
      ],
      "id": "advisory-resort"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "amateur-sharing"
      },
      "source": [
        "c = conv(x)\n",
        "print(c)"
      ],
      "id": "amateur-sharing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "million-liberty"
      },
      "source": [
        "Thus, 1$\\times$1 convolutions can be used to compute weighted sums of input feature maps/channels (in our previous example, all weights were set to 1)."
      ],
      "id": "million-liberty"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "threatened-rolling"
      },
      "source": [
        "## Several output maps\n",
        "Typically, convolutional layer produce several feature maps or channels. For example, consider\n",
        "extending the previous 1$\\times$1 example to two output maps:"
      ],
      "id": "threatened-rolling"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "effective-reform"
      },
      "source": [
        "# 3 input (image) channels, 2 output channel, 1x1 convolution kernel\n",
        "conv = nn.Conv2d(3, 2, 1, bias=False)\n",
        "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
        "print(conv.weight)"
      ],
      "id": "effective-reform",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veterinary-while"
      },
      "source": [
        "This layer maps 3 input feature maps to 2 output feature maps, which are identical in our example, because we initialized all filters so that they are identical:"
      ],
      "id": "veterinary-while"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "civilian-dallas"
      },
      "source": [
        "c = conv(x)\n",
        "print(c)"
      ],
      "id": "civilian-dallas",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "korean-arrow"
      },
      "source": [
        "The first convolutional layer in a network has typically more output feature maps than input channels. Let's assume 3 input channels, 4 output channels of the same dimensionality (i.e., we use padding), and a filter size of 3. For each output channel, we have 3 filter with 9 parameters/weights each. Thus, we have 108 parameters in total:"
      ],
      "id": "korean-arrow"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "infrared-renewal"
      },
      "source": [
        "conv = nn.Conv2d(3, 4, W, padding=W//2, bias=False)\n",
        "print(conv.weight)\n",
        "print(\"Number of parameters:\", conv.weight.shape.numel())"
      ],
      "id": "infrared-renewal",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "primary-chair"
      },
      "source": [
        "And here are the resulting feature maps when applied to our input:"
      ],
      "id": "primary-chair"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blond-damages"
      },
      "source": [
        "c = conv(x)\n",
        "print(c)"
      ],
      "id": "blond-damages",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "central-ozone"
      },
      "source": [
        "# Image processing examples\n",
        "Now we consider a more complex example that involves some basic image transformations. First, we need to import NumPy and some image utilities."
      ],
      "id": "central-ozone"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "preliminary-discount"
      },
      "source": [
        "import torchvision\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "preliminary-discount",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "concerned-demographic"
      },
      "source": [
        "Let's load an image and convert it to grayscale so that we just deal with a single channel. The image is available from Absalon. Please upload it to your google drive folder."
      ],
      "id": "concerned-demographic"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "surprised-advancement"
      },
      "source": [
        "image = Image.open('diku.jpg')  # Load image\n",
        "image = torchvision.transforms.functional.to_grayscale(image)  # Transform to grayscale, because we only wnat one channel"
      ],
      "id": "surprised-advancement",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colonial-netscape"
      },
      "source": [
        "Let's plot the image:"
      ],
      "id": "colonial-netscape"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "median-coordinate"
      },
      "source": [
        "img_np = np.asarray(image)\n",
        "print(\"PIL image shape:\", img_np.shape, \"min:\", img_np.min(), \"max:\", img_np.max())\n",
        "plt.imshow(image, cmap='gray', vmin=0, vmax=255);"
      ],
      "id": "median-coordinate",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "broadband-lingerie"
      },
      "source": [
        "The transformation of the image to a tensor maps has two important effects. First, the values are rescaled to $[0.,1.]$. Second, the channels become the first dimension.  The latter implies that, if we want to plot the image, we have to reorder the axes."
      ],
      "id": "broadband-lingerie"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caroline-strength"
      },
      "source": [
        "x = torchvision.transforms.ToTensor()(image)\n",
        "print(\"Tensor shape:\", x.shape, \"min:\", x.min().item(), \"max:\", x.max().item())\n",
        "plt.imshow(x.permute(1, 2, 0).squeeze(), cmap='gray', vmin=0, vmax=1);"
      ],
      "id": "caroline-strength",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suitable-drill"
      },
      "source": [
        "In order to be process by a layer, the tensor needs  another dimension/axis for enumerating the elements in a batch:"
      ],
      "id": "suitable-drill"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heated-service"
      },
      "source": [
        "x.unsqueeze_(0)  # Add a dimension\n",
        "print(\"Shape after adding batch dimension:\", x.shape);"
      ],
      "id": "heated-service",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "empirical-rainbow"
      },
      "source": [
        "Now we apply a simple horizontal gradient filter:"
      ],
      "id": "empirical-rainbow"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trained-eligibility"
      },
      "source": [
        "hf = torch.tensor([[[[-1., 1.]]]])  # Define filter\n",
        "print(\"Kernel:\", hf, \"shape:\", hf.shape)\n",
        "\n",
        "conv = nn.Conv2d(1, 1, kernel_size=(1, 2), padding=(0, 1), bias=False)  # Padding only in one dimension needed\n",
        "conv.weight = torch.nn.Parameter(hf, requires_grad=False)  # Set kernel parameters to predefined filter parameters\n",
        "c = conv(x)  # Apply filter\n",
        "print(\"Tensor shape:\", c.shape, \"min:\", c.min().item(), \"max:\", c.max().item())"
      ],
      "id": "trained-eligibility",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "historic-comedy"
      },
      "source": [
        "We do not need a gradient for the kernel parameters, so we can use ``requires_grad=False``. This allows us to use ``c[0.0]`` as a NumPy array in the visualizaiton below. Alternatively, we could use ``c[0,0].detach()`` in the ``imshow`` call."
      ],
      "id": "historic-comedy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "natural-trail"
      },
      "source": [
        "print(c[0,0].shape)\n",
        "plt.imshow(c[0,0], cmap='gray', vmin=-1, vmax=1);"
      ],
      "id": "natural-trail",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YpRKMdU02OBx"
      },
      "id": "YpRKMdU02OBx",
      "execution_count": null,
      "outputs": []
    }
  ]
}